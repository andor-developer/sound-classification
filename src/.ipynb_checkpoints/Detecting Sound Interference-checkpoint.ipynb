{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Detecting Sound Interference with Tensorflow for Hangout Sessions\n",
    "By: Andor Kesselman\n",
    "\n",
    "In this analysis, we look at sound interference from multiple hangout streams using google hangout. We train and test the data using a neural network.\n",
    "The goal of this network is to give Google the functionality to enable a \"mute\" action is multiple audio streams occur on the hangouts at the same time. Frequently, when my colleagues and myself use hangouts, we log in at the same time with our computer. The problem is that multiple microphones on at the same time create an issue with loud feedback loops that greatly disrupt a meeting. Furthermore, often it is difficult to determine which of the incoming channels is responsible for disturbing the audio system. \n",
    "We are not audio processing experts, but hope that this simple neural network may provide enough of a baseline to accurately detect multiple audio feedback loops. It would be our hope that it would enable an \"action\" on Google's side, to mute the interefering audio system and prompt a warning. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which type of Classifier do we use? SVM or NN?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we've plotted the values. Now we neeed to do some form of classification. \n",
    "To generate this, we create our own training and test set. \n",
    "First, we generate a single interference instance using google hangouts for ~5 mintues. \n",
    "We then then randomly sample 10 seconds from the 5 minutes.\n",
    "Each sample then input a random noise variant using one of three different methods. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pylab import*\n",
    "from scipy.io import wavfile\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import os\n",
    "from os import listdir\n",
    "from os.path import isfile, join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Define Paths\n",
    "basedir = '/Users/andorkesselman/Documents/rnd/sound/src/'\n",
    "dataset_dir = basedir + 'datasets/'\n",
    "inter_dir_base = dataset_dir + 'base/interference_audio'\n",
    "clean_dir_base = dataset_dir + 'base/clean_audio'\n",
    "inter_dir_gen = dataset_dir + 'generated/interference_audio'\n",
    "clean_dir_gen = dataset_dir + 'generated/clean_audio'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Generation\n",
    "\n",
    "For Dataset generation, we will take existing audio files, induce some noise into the datastream, and then store them locally on the machine. This bootstrapping will allow us to generate lots of training data off of a relatively little sample set. \n",
    "\n",
    "During the training phase, these files will be read and split into 5 fold cross validation set. \n",
    "\n",
    "There are a couple reservations that I have regarding dataset generation. Firstly, my top concern is variety of data. I can add noise, but I do not think there will be enough variety in the datasets to properly train the data. For example, I will not have different languages. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Generate Samples from Base Files and Write them to the Output Directory. \n",
    "# Adjust the parameters to tune the interations\n",
    "\n",
    "def generateSamples(base_dir, out_dir, suffix):\n",
    "    \n",
    "    print('Generating Training Data')\n",
    "    audio_files = listFilesInDirectory(base_dir)\n",
    "    count = 0\n",
    "    iterations_per_file = 5 # the amount of sample files to be generate from each base file\n",
    "    \n",
    "    for file in audio_files:\n",
    "        wav = readWavFile(file)\n",
    "        while (count < iterations_per_file):\n",
    "            sample = sampleFile(wav)\n",
    "            sample = randomNoiseGenerator(sample)\n",
    "            writeSample(sample, join(out_dir + str(count) + \"_\" + suffix + \".wav\"))\n",
    "            count+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Noise Generators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Given a sample, randomly chooses a type of noise to induce on the sample set. \n",
    "# There is a bias toward the original sample type.\n",
    "def randomNoiseGenerator(sample):\n",
    "    r = random.randint(0, 3)\n",
    "    if r == 0:\n",
    "        return whiteNoise(sample)\n",
    "    if r == 1:\n",
    "        x = band_limited_noise(200, 2000, 44100, 44100)\n",
    "        x = np.int16(x * (2**15 - 1))\n",
    "        return x\n",
    "    else:\n",
    "        return sample\n",
    "\n",
    "#Generate White Noise. \n",
    "def whiteNoise(array): \n",
    "    mean = 0\n",
    "    std = 1 \n",
    "    num_samples = 1000\n",
    "    samples = np.random.normal(mean, std, size=num_samples)\n",
    "    return array\n",
    "\n",
    "#Generate Band Limited Noise\n",
    "def fftnoise(f):\n",
    "    f = np.array(f, dtype='complex')\n",
    "    Np = (len(f) - 1) // 2\n",
    "    phases = np.random.rand(Np) * 2 * np.pi\n",
    "    phases = np.cos(phases) + 1j * np.sin(phases)\n",
    "    f[1:Np+1] *= phases\n",
    "    f[-1:-1-Np:-1] = np.conj(f[1:Np+1])\n",
    "    return np.fft.ifft(f).real\n",
    "\n",
    "def band_limited_noise(min_freq, max_freq, samples=sampleFreq, samplerate=1):\n",
    "    freqs = np.abs(np.fft.fftfreq(samples, 1/samplerate))\n",
    "    f = np.zeros(samples)\n",
    "    idx = np.where(np.logical_and(freqs>=min_freq, freqs<=max_freq))[0]\n",
    "    f[idx] = 1\n",
    "    return fftnoise(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## .Wav Support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Read Wav File from Location Specified in Method Call\n",
    "def readWavFile(wavfile_location):\n",
    "    print(\"Reading \" + wavfile_location)\n",
    "    sampleFreq, sample = wavfile.read(wavfile_location)\n",
    "    sample = sample / (2.**15) #normalize and center\n",
    "    ch1 = sample[:,0] #take one channel. There are two channels in this scenario\n",
    "    return ch1\n",
    " \n",
    "# Write Audio Sample To File\n",
    "def writeSample(sample, outdir):\n",
    "    wavfile.write(outdir, 44100, sample)\n",
    "\n",
    "# Randomly Returns a Sample of a File. TODO: Improve Sampling Method. \n",
    "# Must Remain a Sequence in this Case because Audio is time dependent. Randomly Sampling would be BAD.\n",
    "def sampleFile(wav):\n",
    "    #take random \n",
    "    size = int(len(wav) / 5) # take 1/5 of the full file size. TODO: What's the best implemenation?\n",
    "    i = int(len(wav) - size - 1)\n",
    "    r = random.randint(0, i) # make sure that we get a full set. Hence the - size\n",
    "    return wav[r:(r+size), ]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Only store the name. Do not store the file, as that would remain in memory.\n",
    "def listFilesInDirectory(directory):\n",
    "    ret = []\n",
    "    for file in listdir(directory):\n",
    "            ret.append(join(directory, file))\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training\n",
    "\n",
    "In the training data, we will split up all the files in the training set into 5 k-folds, with 3 kfolds for the original training data, 1 k fold for the first test, and then the final validation set.\n",
    "\n",
    "CNN\n",
    "http://yerevann.github.io/2015/10/11/spoken-language-identification-with-deep-convolutional-networks/\n",
    "http://research.microsoft.com/en-us/um/people/dongyu/nips2009/papers/montavon-paper.pdf\n",
    "\n",
    "SVM\n",
    "http://www.ee.columbia.edu/~sfchang/course/spr-F05/papers/guo-li-svm-audio00.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Feature Extraction\n",
    "def ExtractFeatures(sample):\n",
    "    print('Feature Extraction. Potentially use PCA')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Round 1: NN\n",
    "class NeuralNetwork():\n",
    "    print('This is the Neural Network')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Round 2: SVM\n",
    "class SVM():\n",
    "    print('This is an SVM approach')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating Training Data\n",
      "Reading /Users/andorkesselman/Documents/rnd/sound/src/datasets/base/clean_audio/1_clean.wav\n",
      "Adding white noise to the sample set\n",
      "Generating Training Data\n",
      "Reading /Users/andorkesselman/Documents/rnd/sound/src/datasets/base/interference_audio/1_inter.wav\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Unknown wave file format",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-147-10ac5c5ff177>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mgenerateSamples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclean_dir_base\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mclean_dir_gen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"clean\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mgenerateSamples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minter_dir_base\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0minter_dir_gen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"inter\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-146-ca8fda29e92e>\u001b[0m in \u001b[0;36mgenerateSamples\u001b[0;34m(base_dir, out_dir, suffix)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mfile\u001b[0m \u001b[0;32min\u001b[0m \u001b[0maudio_files\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0mwav\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreadWavFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mcount\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0miterations_per_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m             \u001b[0msample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msampleFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwav\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-140-31655978c9f9>\u001b[0m in \u001b[0;36mreadWavFile\u001b[0;34m(wavfile_location)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mreadWavFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwavfile_location\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Reading \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mwavfile_location\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0msampleFreq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwavfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwavfile_location\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0msample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msample\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m2.\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;36m15\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#normalize and center\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mch1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msample\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m#take one channel. There are two channels in this scenario\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/andorkesselman/anaconda3/lib/python3.5/site-packages/scipy/io/wavfile.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(filename, mmap)\u001b[0m\n\u001b[1;32m    244\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mchunk_id\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34mb'fmt '\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    245\u001b[0m                 \u001b[0mfmt_chunk_received\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 246\u001b[0;31m                 \u001b[0mfmt_chunk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_read_fmt_chunk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_big_endian\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    247\u001b[0m                 \u001b[0mformat_tag\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchannels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfmt_chunk\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    248\u001b[0m                 \u001b[0mbit_depth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfmt_chunk\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/andorkesselman/anaconda3/lib/python3.5/site-packages/scipy/io/wavfile.py\u001b[0m in \u001b[0;36m_read_fmt_chunk\u001b[0;34m(fid, is_big_endian)\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mformat_tag\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mKNOWN_WAVE_FORMATS\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Unknown wave file format\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m     \u001b[0;31m# move file pointer to next chunk\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Unknown wave file format"
     ]
    }
   ],
   "source": [
    "generateSamples(clean_dir_base,clean_dir_gen, \"clean\")\n",
    "generateSamples(inter_dir_base,inter_dir_gen, \"inter\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
