{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Detecting Sound Interference with Tensorflow/SKLearn for Hangout Sessions\n",
    "By: Andor Kesselman\n",
    "\n",
    "In this analysis, we look at sound interference from multiple hangout streams using google hangout. We train and test the data using a neural network.\n",
    "The goal of this network is to give Google the functionality to enable a \"mute\" action is multiple audio streams occur on the hangouts at the same time. Frequently, when my colleagues and myself use hangouts, we log in at the same time with our computer. The problem is that multiple microphones on at the same time create an issue with loud feedback loops that greatly disrupt a meeting. Furthermore, often it is difficult to determine which of the incoming channels is responsible for disturbing the audio system. \n",
    "We are not audio processing experts, but hope that this simple neural network may provide enough of a baseline to accurately detect multiple audio feedback loops. It would be our hope that it would enable an \"action\" on Google's side, to mute the interefering audio system and prompt a warning. \n",
    "\n",
    "\n",
    "Most of the audio processing in this is not novel. I have very little idea how audio processing works. Much of these methods, use concepts that I am just brushing up upon, including a fourier transform, zero cross rates, and many more. There are two relatively newer ideas implemented in this paper .\n",
    "\n",
    "Idea 1: Neural Network Approach vs. SVM vs. NN (problem with too high dimensionality)\n",
    "\n",
    "Idea 2: Dataset generation bootstraps datapoints."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which type of Classifier do we use? SVM or NN?\n",
    "\n",
    "### Note: For Prototyping purposes, I'm going to be using PyAudio from\n",
    "https://github.com/tyiannak/pyAudioAnalysis\n",
    "\n",
    "https://github.com/tracek/Ornithokrites\n",
    "\n",
    "I would like to re-evaluate the classification algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we've plotted the values. Now we neeed to do some form of classification. \n",
    "To generate this, we create our own training and test set. \n",
    "First, we generate a single interference instance using google hangouts for ~5 mintues. \n",
    "We then then randomly sample 10 seconds from the 5 minutes.\n",
    "Each sample then input a random noise variant using one of three different methods. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Helpers\n",
    "import random\n",
    "import os\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import sys\n",
    "import uuid\n",
    "\n",
    "#Plotting and Math\n",
    "from pylab import*\n",
    "from scipy.io import wavfile\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from numpy.lib import stride_tricks\n",
    "\n",
    "\n",
    "# Classification and evaluation\n",
    "import sklearn\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import svm\n",
    "from sklearn.cross_validation import StratifiedKFold, ShuffleSplit, cross_val_score\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# Audio Libraries\n",
    "#import audioFeatureExtraction\n",
    "#import audioTrainTest as aT\n",
    "\n",
    "#Debug\n",
    "from pprint import pprint\n",
    "\n",
    "sys.path.insert(0, './lib') #insert lib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Define Paths\n",
    "basedir = '/Users/andorkesselman/Documents/rnd/sound/src/'\n",
    "dataset_dir = basedir + 'datasets/'\n",
    "inter_dir_base = dataset_dir + 'base/interference_audio'\n",
    "clean_dir_base = dataset_dir + 'base/clean_audio'\n",
    "generated_dir = dataset_dir + 'generated/'\n",
    "base_dataset = dataset_dir + 'base/'\n",
    "inter_dir_gen = dataset_dir + 'generated/interference_audio/'\n",
    "clean_dir_gen = dataset_dir + 'generated/clean_audio/'\n",
    "PLOT_WIDTH  = 15\n",
    "PLOT_HEIGHT = 3.5\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Generation And Reading\n",
    "\n",
    "For Dataset generation, we will take existing audio files, induce some noise into the datastream, and then store them locally on the machine. This bootstrapping will allow us to generate lots of training data off of a relatively little sample set. \n",
    "\n",
    "During the training phase, these files will be read and split into 5 fold cross validation set. \n",
    "\n",
    "There are a couple reservations that I have regarding dataset generation. Firstly, my top concern is variety of data. I can add noise, but I do not think there will be enough variety in the datasets to properly train the data. For example, I will not have different languages and there are a variety of settings at which this will not work. \n",
    "\n",
    "Where we improve the existing models I have seen is how we store the feature data. We will not store the actual waveform data, as that will be too. To reduce the amount of memory consumption, we will store the feature data (not the actual waveforms, on file to be read into memory). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\" type: map\n",
    "Sound File Storage: Useful for sound storage. Try to only store meta data if possible. All these are stores in a Features Objects. \n",
    "soundfiles[id] \n",
    "      -> label\n",
    "      -> sampleRate\n",
    "      -> fileLocation\n",
    "      -> zcr\n",
    "      -> waveFormData // This has to be stored in memory. Need to adjust moving forward. \n",
    "      -> rms\n",
    "      -> spectralcentroid\n",
    "      -> spectralrolloff\n",
    "      -> spetral flux\n",
    "\"\"\"\n",
    "\n",
    "soundfiles = {}\n",
    "\n",
    "class Features(object):   \n",
    "    def __init__(self):\n",
    "        pass \n",
    "\"\"\"\n",
    "Label Storage: Useful for quick lookup. \n",
    "labelmap[label] -> list<id's with label>\n",
    "\"\"\"\n",
    "labelmap = {}\n",
    "\n",
    "\"\"\"\n",
    "Internal Mapping:\n",
    "labelmap[id] -> soundfiles[id] \n",
    "\"\"\"\n",
    "\n",
    "def generateSamples(base_dir, out_dir, suffix):\n",
    "    \n",
    "    print('Generating Training Data')\n",
    "    audio_files = listFilesInDirectory(base_dir)\n",
    "    count = 0\n",
    "    iterations_per_file = 5 # the amount of sample files to be generate from each base file\n",
    "    \n",
    "    for file in audio_files:\n",
    "        wav, sampleFreq = readWavFile(file)\n",
    "        while (count < iterations_per_file):\n",
    "            sample = sampleFile(wav)\n",
    "            sample = randomNoiseGenerator(sample, sampleFreq)\n",
    "            writeSample(sample, join(out_dir + str(count) + \"_\" + suffix + \".wav\")) #write the sample and after adding noise. Naming convention malleable. \n",
    "            count+=1\n",
    "            \n",
    "            \n",
    "\"\"\" Toss in the generated datasets. These are the final datasets. \"\"\"\n",
    "def readSamples(dataset_dir):\n",
    "\n",
    "    # Store classification in folders\n",
    "    for subdir in listSubDirectories(dataset_dir):\n",
    "        spldir = subdir.split(\"/\")\n",
    "        classifier = spldir[len(spldir) - 1]\n",
    "        print(\"Working on \" + classifier  + \" classifier right now ... Please wait\")\n",
    "        for file in listFilesInDirectory(subdir):\n",
    "           \n",
    "            # Check for collisions and create UUID\n",
    "            uuid = generateUUID() \n",
    "            if uuid in soundfiles: #upon collision create new UUID. Assumes no second collision. \n",
    "                uuid = generateUUID() \n",
    "          \n",
    "            #Add Features\n",
    "            features = Features()\n",
    "            features.label = classifier \n",
    "            features.fileLocation = file\n",
    "            features.uuid = uuid\n",
    "            fe = FeatureExtractor(features) \n",
    "            features = fe.extractFeatures()\n",
    "            \n",
    "            # Add Mapping for Easy LookUP of Classification to ID\n",
    "            if features.label not in labelmap:\n",
    "                idlist = []\n",
    "                idlist.append(uuid)\n",
    "                labelmap[str(features.label)] = idlist\n",
    "            else:\n",
    "                idlist = labelmap[str(features.label)]\n",
    "                idlist.append(uuid) #add uuid\n",
    "                labelmap[str(features.label)] = idlist\n",
    "                \n",
    "            soundfiles[uuid] = features  \n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Noise Generators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "The point of this is to introduce random noise variables to bootstrap the dataset. \n",
    "\"\"\"\n",
    "def randomNoiseGenerator(sample, sampleFreq):\n",
    "    r = random.randint(0, 3)\n",
    "    if r == 0:\n",
    "        return whiteNoise(sample)\n",
    "    if r == 1:\n",
    "        x = band_limited_noise(200, 2000, 44100, 44100)\n",
    "        x = np.int16(x * (2**15 - 1))\n",
    "        return x\n",
    "    else:\n",
    "        return sample\n",
    "\n",
    "#Generate White Noise. \n",
    "def whiteNoise(array): \n",
    "    mean = 0\n",
    "    std = 1 \n",
    "    num_samples = 1000\n",
    "    samples = np.random.normal(mean, std, size=num_samples)\n",
    "    return array\n",
    "\n",
    "#Generate Band Limited Noise\n",
    "def fftnoise(f):\n",
    "    f = np.array(f, dtype='complex')\n",
    "    Np = (len(f) - 1) // 2\n",
    "    phases = np.random.rand(Np) * 2 * np.pi\n",
    "    phases = np.cos(phases) + 1j * np.sin(phases)\n",
    "    f[1:Np+1] *= phases\n",
    "    f[-1:-1-Np:-1] = np.conj(f[1:Np+1])\n",
    "    return np.fft.ifft(f).real\n",
    "\n",
    "def band_limited_noise(min_freq, max_freq, samples, samplerate=1):\n",
    "    freqs = np.abs(np.fft.fftfreq(samples, 1/samplerate))\n",
    "    f = np.zeros(samples)\n",
    "    idx = np.where(np.logical_and(freqs>=min_freq, freqs<=max_freq))[0]\n",
    "    f[idx] = 1\n",
    "    return fftnoise(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## .Wav Support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Read Wav File from Location Specified in Method Call\n",
    "def readWavFile(wavfile_location):\n",
    "    sampleFreq, sample = wavfile.read(wavfile_location)\n",
    "    sample = sample / (2.**15) #normalize and center\n",
    "    ch1 = sample[:,0] #take one channel. There are two channels in this scenario\n",
    "    return ch1, sampleFreq\n",
    "\n",
    "# Read Wav File from Location Specified in Method Call\n",
    "def readWavFileAllChannels(wavfile_location):\n",
    "    sampleFreq, sample = wavfile.read(wavfile_location)\n",
    "    sample = sample / (2.**15) #normalize and center\n",
    "    ch1 = sample[:,0] #take one channel. There are two channels in this scenario\n",
    "    return sample, sampleFreq\n",
    " \n",
    "# Write Audio Sample To File\n",
    "def writeSample(sample, outdir):\n",
    "    wavfile.write(outdir, 44100, sample)\n",
    "\n",
    "\"\"\" Randomly Returns a Sample of a File. TODO: Improve Sampling Method.  \n",
    "Must Remain a Sequence in this Case because Audio is time dependent. Randomly Sampling would be BAD. \"\"\"\n",
    "def sampleFile(wav):\n",
    "    #take random \n",
    "    size = int(len(wav) / 5) # take 1/5 of the full file size. TODO: What's the best implemenation?\n",
    "    i = int(len(wav) - size - 1)\n",
    "    r = random.randint(0, i) # make sure that we get a full set. Hence the - size\n",
    "    return wav[r:(r+size), ]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Extraction\n",
    "\n",
    "OK, going into the project, I thought feature extraction would be relatively simple. I was thinking of just doing something like a nearest neighbor approach using the amplitude and frequencies of the wave as the feature. I quickly found out that to not be the case. This is complicated, and I relied heavily on prior work: http://www.ifs.tuwien.ac.at/~schindler/lectures/MIR_Feature_Extraction.html for guidance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "Features Extraction, gives us the data to store things like the zcr, waveformdata, rms, etc into a feature object. \n",
    "type: map\n",
    "Sound File Storage: Useful for sound storage. Try to only store meta data if possible. All these are stores in a Features Objects. \n",
    "soundfiles[id] \n",
    "      -> label\n",
    "      -> sampleRate\n",
    "      -> fileLocation\n",
    "      -> zcr\n",
    "      -> waveFormData // This has to be stored in memory. Need to adjust moving forward. \n",
    "      -> rms\n",
    "      -> spectralcentroid\n",
    "      -> spectralrolloff\n",
    "      -> spetral flux\n",
    "\"\"\"\n",
    "class FeatureExtractor(object):\n",
    "\n",
    "    def __init__(self, features):\n",
    "        self.features = features\n",
    "        \n",
    "    def extractFeatures(self):\n",
    "        waveform, sampleRate = readWavFileAllChannels(self.features.fileLocation)\n",
    "        self.features.waveform = waveform\n",
    "        self.features.sampleRate = sampleRate\n",
    "        \n",
    "        blocklength = 2048\n",
    "       # self.features.zcr = zero_crossing_rate(waveform, blocklength, sampleRate)\n",
    "       # self.features.rms = root_mean_square(waveform, blocklength, sampleRate)\n",
    "       # self.features.spectral_centroid = spectral_centroid(waveform, blocklength, sampleRate)\n",
    "       # self.features.spectral_rolloff = spectral_rolloff(waveform, blocklength, sampleRate)\n",
    "      #  self.features.spectral_flux = spectral_flux(waveform, blocklength, sampleRate)\n",
    "\n",
    "        return self.features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Feature Extractor\n",
      "{'fileLocation': '/Users/andorkesselman/Documents/rnd/sound/src/datasets/base/clean_audio/1_clean.wav',\n",
      " 'sampleRate': 44100,\n",
      " 'waveform': array([[ 0.,  0.],\n",
      "       [ 0.,  0.],\n",
      "       [ 0.,  0.],\n",
      "       ..., \n",
      "       [ 0.,  0.],\n",
      "       [ 0.,  0.],\n",
      "       [ 0.,  0.]])}\n",
      "---------FEATURE EXTRATOR PASSED----------\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Simple check to make sure the feature extrator is working. Should be run only once.\"\"\"\n",
    "def testFeatureExtraction():\n",
    "    print(\"Testing Feature Extractor\")\n",
    "    fileloc = '/Users/andorkesselman/Documents/rnd/sound/src/datasets/base/clean_audio/1_clean.wav'\n",
    "    wav = readWavFile(fileloc)\n",
    "    features = Features()\n",
    "    features.fileLocation = fileloc\n",
    "    features = FeatureExtractor(features).extractFeatures()\n",
    "    pprint(vars(features))\n",
    "    print(\"---------FEATURE EXTRATOR PASSED----------\")\n",
    "\n",
    "testFeatureExtraction()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transform Functions\n",
    "\n",
    "Note: Most of these functions were taken from:\n",
    "\n",
    "http://www.ifs.tuwien.ac.at/~schindler/lectures/MIR_Feature_Extraction.html. \n",
    "\n",
    "One could argue that we actually don't need this to accomplish our tasks. After all, we could probably take a nearest neighbor on a block by block basis to classify. This could be considered excessive. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\" Short time forier transform \"\"\"\n",
    "def stft(sig, frameSize, overlapFac=0.5, window=np.hanning):\n",
    "    \n",
    "    # get the frame size and the hopsize\n",
    "    win = window(frameSize)\n",
    "    hopSize = int(frameSize - np.floor(overlapFac * frameSize))\n",
    "    \n",
    "    # zeros at beginning (thus center of 1st window should be for sample nr. 0)\n",
    "    samples = np.append(np.zeros(np.floor(frameSize/2.0)), sig)    \n",
    "    # cols for windowing\n",
    "    cols = np.ceil( (len(samples) - frameSize) / float(hopSize)) + 1\n",
    "    # zeros at end (thus samples can be fully covered by frames)\n",
    "    samples = np.append(samples, np.zeros(frameSize))\n",
    "    \n",
    "    frames = np.lib.stride_tricks.as_strided(samples, shape=(cols, frameSize), strides=(samples.strides[0]*hopSize, samples.strides[0])).copy()\n",
    "    frames *= win\n",
    "    \n",
    "    #use numpys computes the one-dimensional discrete Fourier Transform for real input.\n",
    "    return np.fft.rfft(frames)    \n",
    "\n",
    "\n",
    "\"\"\" scale frequency axis logarithmically. Not sure exactly how this function works. Need to revisit in due time. \"\"\"    \n",
    "def logscale_spec(spec, sr=44100, factor=20.): # TODO: The sample rate needs to be adjusted to the sample rate of the music\n",
    "    timebins, freqbins = np.shape(spec) #Get the shape of the spec\n",
    "    print(\"TimeBins are \" + timebins + \" Frequency Bins are \" + freqbins)\n",
    "    scale = np.linspace(0, 1, freqbins) ** factor  # what is the factor?\n",
    "    scale *= (freqbins-1)/max(scale) #Scale values for bins. \n",
    "    scale = np.unique(np.round(scale)) #Round the interger values\n",
    "    \n",
    "    # create spectrogram with new freq bins\n",
    "    newspec = np.complex128(np.zeros([timebins, len(scale)])) #real and complex number\n",
    "    for i in range(0, len(scale)):\n",
    "        if i == len(scale)-1:\n",
    "            newspec[:,i] = np.sum(spec[:,scale[i]:], axis=1) \n",
    "        else:        \n",
    "            newspec[:,i] = np.sum(spec[:,scale[i]:scale[i+1]], axis=1)\n",
    "    \n",
    "    # list center freq of bins\n",
    "    allfreqs = np.abs(np.fft.fftfreq(freqbins*2, 1./sr)[:freqbins+1])\n",
    "    freqs = []\n",
    "    for i in range(0, len(scale)):\n",
    "        if i == len(scale)-1:\n",
    "            freqs += [np.mean(allfreqs[scale[i]:])]\n",
    "        else:\n",
    "            freqs += [np.mean(allfreqs[scale[i]:scale[i+1]])]\n",
    "            \n",
    "    \n",
    "    return newspec, freqs\n",
    "\n",
    "\n",
    "\"\"\"The following method is numerical more elegant and computationally efficient. It uses the optimized linear algebraic functions of the Numerical Python (numpy) package.\n",
    "The method further computes the Zero Crossing Rate for a sequence of blocks (also called frames or windows).\"\"\"\n",
    "def zero_crossing_rate(wavedata, block_length, sample_rate):\n",
    "    \n",
    "    # how many blocks have to be processed?\n",
    "    num_blocks = int(np.ceil(len(wavedata)/block_length))\n",
    "    \n",
    "    # when do these blocks begin (time in seconds)?\n",
    "    timestamps = (np.arange(0,num_blocks - 1) * (block_length / float(sample_rate)))\n",
    "    \n",
    "    zcr = []\n",
    "    \n",
    "    for i in range(0,num_blocks-1):\n",
    "        \n",
    "        start = i * block_length\n",
    "        stop  = np.min([(start + block_length - 1), len(wavedata)])\n",
    "        zc = 0.5 * np.mean(np.abs(np.diff(np.sign(wavedata[start:stop]))))\n",
    "        zcr.append(zc)\n",
    "    \n",
    "    return np.asarray(zcr), np.asarray(timestamps)\n",
    "\n",
    "\"\"\" Root mean squared calculates the relative energy. It uses the amplitude, but calculates the instantaneous energy. \"\"\"\n",
    "def root_mean_square(wavedata, block_length, sample_rate):\n",
    "    \n",
    "    # how many blocks have to be processed?\n",
    "    num_blocks = int(np.ceil(len(wavedata)/block_length))\n",
    "    \n",
    "    # when do these blocks begin (time in seconds)?\n",
    "    timestamps = (np.arange(0,num_blocks - 1) * (block_length / float(sample_rate)))\n",
    "    \n",
    "    rms = []\n",
    "    \n",
    "    for i in range(0,num_blocks-1):\n",
    "        \n",
    "        start = i * block_length\n",
    "        stop  = np.min([(start + block_length - 1), len(wavedata)])\n",
    "        \n",
    "        rms_seg = np.sqrt(np.mean(wavedata[start:stop]**2))\n",
    "        rms.append(rms_seg)\n",
    "    \n",
    "    return np.asarray(rms), np.asarray(timestamps)\n",
    "\n",
    "\"\"\" Determines where the concentration of energy occurs. \"\"\"\n",
    "def spectral_centroid(wavedata, window_size, sample_rate):\n",
    "    \n",
    "    magnitude_spectrum = stft(wavedata, window_size)\n",
    "    timebins, freqbins = np.shape(magnitude_spectrum)\n",
    "    \n",
    "    # when do these blocks begin (time in seconds)?\n",
    "    timestamps = (np.arange(0,timebins - 1) * (timebins / float(sample_rate)))\n",
    "    \n",
    "    sc = []\n",
    "\n",
    "    for t in range(timebins-1):\n",
    "        power_spectrum = np.abs(magnitude_spectrum[t])**2      \n",
    "        sc_t = np.sum(power_spectrum * np.arange(1,freqbins+1)) / np.sum(power_spectrum)\n",
    "        sc.append(sc_t)\n",
    "    \n",
    "    sc = np.asarray(sc)\n",
    "    sc = np.nan_to_num(sc)\n",
    "    \n",
    "    return sc, np.asarray(timestamps)\n",
    "\n",
    "\n",
    "\"\"\" Computes the spectral rolloff which measures the skewness of the spectral shape. \"\"\"\n",
    "def spectral_rolloff(wavedata, window_size, sample_rate, k=0.85):\n",
    "    \n",
    "    # convert to frequency domain\n",
    "    magnitude_spectrum = stft(wavedata, window_size)\n",
    "    power_spectrum     = np.abs(magnitude_spectrum)**2\n",
    "    timebins, freqbins = np.shape(magnitude_spectrum)\n",
    "    \n",
    "    # when do these blocks begin (time in seconds)?\n",
    "    timestamps = (np.arange(0,timebins - 1) * (timebins / float(sample_rate)))\n",
    "    \n",
    "    sr = []\n",
    "    spectralSum    = np.sum(power_spectrum, axis=1)\n",
    "    \n",
    "    for t in range(timebins-1):\n",
    "        \n",
    "        # find frequency-bin indeces where the cummulative sum of all bins is higher\n",
    "        # than k-percent of the sum of all bins. Lowest index = Rolloff\n",
    "        sr_t = np.where(np.cumsum(power_spectrum[t,:]) >= k * spectralSum[t])[0][0]\n",
    "        \n",
    "        sr.append(sr_t)\n",
    "        \n",
    "    sr = np.asarray(sr).astype(float)\n",
    "    \n",
    "    # convert frequency-bin index to frequency in Hz\n",
    "    sr = (sr / freqbins) * (sample_rate / 2.0)\n",
    "    \n",
    "    return sr, np.asarray(timestamps)\n",
    "\n",
    "\"\"\" Measures the rate of local change\"\"\"\n",
    "def spectral_flux(wavedata, window_size, sample_rate):\n",
    "    \n",
    "    # convert to frequency domain\n",
    "    magnitude_spectrum = stft(wavedata, window_size)\n",
    "    timebins, freqbins = np.shape(magnitude_spectrum)\n",
    "    \n",
    "    # when do these blocks begin (time in seconds)?\n",
    "    timestamps = (np.arange(0,timebins - 1) * (timebins / float(sample_rate)))\n",
    "    \n",
    "    sf = np.sqrt(np.sum(np.diff(np.abs(magnitude_spectrum))**2, axis=1)) / freqbins\n",
    "    \n",
    "    return sf[1:], np.asarray(timestamps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utility\n",
    "\n",
    "### Graphical Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\" Shows the stereo waveform of both channels. \"\"\"\n",
    "def show_stereo_waveform(samples):\n",
    "\n",
    "\tfig = plt.figure(num=None, figsize=(PLOT_WIDTH, 5), dpi=72, facecolor='w', edgecolor='k')\n",
    "\n",
    "\tchannel_1 = fig.add_subplot(211)\n",
    "\tchannel_1.set_ylabel('Channel 1')\n",
    "\tchannel_1.set_xlim(0,len(samples)) \n",
    "\tchannel_1.set_ylim(-1,1)\n",
    "\tchannel_1.plot(samples[:,0])\n",
    "\n",
    "\tchannel_2 = fig.add_subplot(212)\n",
    "\tchannel_2.set_ylabel('Channel 2')\n",
    "\tchannel_2.set_xlabel('Time (s)')\n",
    "\tchannel_2.set_ylim(-1, 1)\n",
    "\tchannel_2.set_xlim(0,len(samples)) \n",
    "\tchannel_2.plot(samples[:,1])\n",
    "\n",
    "\tplt.show();\n",
    "\tplt.clf();\n",
    "    \n",
    "\n",
    "\"\"\"  Takes the mean of both channels and outputs it as the mono wave form. \"\"\"\n",
    "def show_mono_waveform(samples):\n",
    "\n",
    "\tfig = plt.figure(num=None, figsize=(PLOT_WIDTH, PLOT_HEIGHT), dpi=72, facecolor='w', edgecolor='k')\n",
    "\n",
    "\tchannel_1 = fig.add_subplot(111)\n",
    "\tchannel_1.set_ylabel('Channel 1')\n",
    "\tchannel_1.set_xlim(0,len(samples)) \n",
    "\tchannel_1.set_ylim(-1,1)\n",
    "\n",
    "\tchannel_1.plot(samples)\n",
    "\n",
    "\tplt.show();\n",
    "\tplt.clf();\n",
    "    \n",
    "\n",
    "\"\"\" This is a spectogram plot. Please see: https://en.wikipedia.org/wiki/Spectrogram for more information. \"\"\"\n",
    "def plotstft(samples, samplerate, binsize=2**10, plotpath=None, colormap=\"jet\", ax=None, fig=None):\n",
    "    \n",
    "    # Get the 1 dimensional Short Fourier Transform using the numpy's ftf function \n",
    "    s = stft(samples, binsize)\n",
    "    \n",
    "    #move the ferequence to log scale. \n",
    "    sshow, freq = logscale_spec(s, factor=1.0, sr=samplerate)\n",
    "    ims = 20.*np.log10(np.abs(sshow)/10e-6) # amplitude to decibel\n",
    "    \n",
    "    timebins, freqbins = np.shape(ims)\n",
    "    \n",
    "    if ax is None:\n",
    "        fig, ax = plt.subplots(1, 1, sharey=True, figsize=(PLOT_WIDTH, 3.5))\n",
    "    \n",
    "    #ax.figure(figsize=(15, 7.5))\n",
    "    cax = ax.imshow(np.transpose(ims), origin=\"lower\", aspect=\"auto\", cmap=colormap, interpolation=\"none\")\n",
    "    #cbar = fig.colorbar(cax, ticks=[-1, 0, 1], cax=ax)\n",
    "    #ax.set_colorbar()\n",
    "\n",
    "    #label\n",
    "    ax.set_xlabel(\"time (s)\")\n",
    "    ax.set_ylabel(\"frequency (hz)\")\n",
    "    ax.set_xlim([0, timebins-1])\n",
    "    ax.set_ylim([0, freqbins])\n",
    "\n",
    "    xlocs = np.float32(np.linspace(0, timebins-1, 5))\n",
    "    ax.set_xticks(xlocs, [\"%.02f\" % l for l in ((xlocs*len(samples)/timebins)+(0.5*binsize))/samplerate])\n",
    "    ylocs = np.int16(np.round(np.linspace(0, freqbins-1, 10)))\n",
    "    ax.set_yticks(ylocs, [\"%.02f\" % freq[i] for i in ylocs])\n",
    "    \n",
    "    if plotpath:\n",
    "        plt.savefig(plotpath, bbox_inches=\"tight\")\n",
    "    else:\n",
    "        plt.show()\n",
    "        \n",
    "    #plt.clf();\n",
    "    b = [\"%.02f\" % l for l in ((xlocs*len(samples)/timebins)+(0.5*binsize))/samplerate]\n",
    "    return xlocs, b, timebins\n",
    "\n",
    "\"\"\" Show super imposed feature set\n",
    "TODO: This feature will not work yet because lookup is not working. \"\"\"\n",
    "def show_feature_superimposed(genre, feature_data, timestamps, squared_wf=False):\n",
    "    \n",
    "    # plot spectrogram. Get all the sounds files and sample rate for each of the files. Returns the xlocs, b, and timebins. \n",
    "    a,b,c = plotstft(sound_files[genre][\"wavedata\"], sound_files[genre][\"samplerate\"]);\n",
    "\n",
    "    fig = plt.figure(num=None, figsize=(PLOT_WIDTH, PLOT_HEIGHT), dpi=72, facecolor='w', edgecolor='k');\n",
    "    channel_1 = fig.add_subplot(111);\n",
    "    channel_1.set_ylabel('Channel 1');\n",
    "    channel_1.set_xlabel('time');\n",
    "\n",
    "    # plot waveform\n",
    "    scaled_wf_y = ((np.arange(0,sound_files[genre][\"wavedata\"].shape[0]).astype(np.float)) / sound_files[genre][\"samplerate\"]) * 1000.0\n",
    "    \n",
    "    if squared_wf:\n",
    "        scaled_wf_x = (sound_files[genre][\"wavedata\"]**2 / np.max(sound_files[genre][\"wavedata\"]**2))\n",
    "    else:\n",
    "        scaled_wf_x = (sound_files[genre][\"wavedata\"] / np.max(sound_files[genre][\"wavedata\"]) / 2.0 ) + 0.5\n",
    "    \n",
    "    #scaled_wf_x = scaled_wf_x**2\n",
    "    \n",
    "    plt.plot(scaled_wf_y, scaled_wf_x, color='lightgrey');\n",
    "\n",
    "    # plot feature-data\n",
    "    scaled_fd_y = timestamps * 1000.0\n",
    "    scaled_fd_x = (feature_data / np.max(feature_data))\n",
    "    \n",
    "    plt.plot(scaled_fd_y, scaled_fd_x, color='r');\n",
    "\n",
    "    plt.show();\n",
    "    plt.clf();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### File Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\" List the files in the directory \"\"\"\n",
    "def listFilesInDirectory(directory):\n",
    "    ret = []\n",
    "    for file in listdir(directory):\n",
    "            ret.append(join(directory, file))\n",
    "    return ret\n",
    "\n",
    "\"\"\" List the directories in a parent directory. Returns a list of directories. \"\"\"\n",
    "def listSubDirectories(directory):\n",
    "    ret = []\n",
    "    for subdirectory in listdir(directory):\n",
    "            path = join(directory, subdirectory)\n",
    "            if(os.path.isdir(path)):\n",
    "                ret.append(path)\n",
    "    return ret\n",
    "\n",
    "\"\"\" Generate Random UUID \"\"\"\n",
    "def generateUUID():\n",
    "    return uuid.uuid4() \n",
    "\n",
    "def testFileHelpers():\n",
    "    print(\"Listing Files in Directory\")\n",
    "    print(listFilesInDirectory(inter_dir_gen))\n",
    "    print(\"Listing SubDirectories\")\n",
    "    print(listSubDirectories(dataset_dir))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifiers\n",
    "\n",
    "In the training data, we will split up all the files in the training set into 5 k-folds, with 3 kfolds for the original training data, 1 k fold for parameter turning, and then the final validation set.\n",
    "\n",
    "CNN\n",
    "http://yerevann.github.io/2015/10/11/spoken-language-identification-with-deep-convolutional-networks/\n",
    "http://research.microsoft.com/en-us/um/people/dongyu/nips2009/papers/montavon-paper.pdf\n",
    "\n",
    "SVM\n",
    "http://www.ee.columbia.edu/~sfchang/course/spr-F05/papers/guo-li-svm-audio00.pdf\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is the Neural Network\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Need to build the Nueral Network \"\"\"\n",
    "class NeuralNetwork():\n",
    "    print('This is the Neural Network')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is an SVM approach\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\"\"\" SVM Approach has a couple of weaknesses. \n",
    "\n",
    "Advantages:\n",
    "  1) Regularlization\n",
    "  2) No local minima\n",
    "  \n",
    "Disadvantages:\n",
    "  1) Kernel choice \n",
    "  2) Speed \n",
    "\n",
    "\"\"\"\n",
    "\n",
    "class SVM():\n",
    "\n",
    "    def __init__(self, X, labels):\n",
    "        print(\" Selected SVM approach. \")\n",
    "        if(len(X) != len(labels)):\n",
    "            print(\"X must be the same size of the labels. Not matching.\")\n",
    "            print(\"Please input fields such as X = [[0, 0], [1, 1]], labels = [0, 1]\")\n",
    "            print(\"Note that both of these are length two\")\n",
    "        \n",
    "        self.X = X\n",
    "        self.labels = labels\n",
    "    \n",
    "    def Fit(self):\n",
    "        svm_classifier = svm.SVC()\n",
    "        self.svm = svm_classifier\n",
    "        svm_classifier.fit(self.X, self.labels)  \n",
    "    \n",
    "    def Predict(X):\n",
    "        prediction = self.svm_classifier.predict(X)\n",
    "        print(\"Predicted \", prediction , \" with SVM Classifier and Input \", X)\n",
    "        return prediction\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'features' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-50-9b7dff693ae1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;34m\"\"\" For the Nearest Neighbor Calculation. We are going to use the Eucliedean L2 Distance Calclation as it is less biased to outliers\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mKNearestNeighbor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_neighbors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-50-9b7dff693ae1>\u001b[0m in \u001b[0;36mKNearestNeighbor\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;31m# calculate Euclidean Distance\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     distances_l2 = np.sqrt((((features[feature_set][\"data\"] - \n\u001b[0m\u001b[1;32m      8\u001b[0m                      features[feature_set][\"data\"][cur_instance,:]))**2\n\u001b[1;32m      9\u001b[0m              ).sum(axis=1))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'features' is not defined"
     ]
    }
   ],
   "source": [
    "\"\"\" For the Nearest Neighbor Calculation. \n",
    "    We are going to use the Eucliedean L2 Distance Calclation \n",
    "    as it is less biased to outliers. \"\"\"\n",
    "\n",
    "class KNearestNeighbor():\n",
    "    \n",
    "    def __init__(self, n_neighbors):\n",
    "        pass\n",
    "    \n",
    "    # calculate Euclidean Distance\n",
    "    distances_l2 = np.sqrt((((features[feature_set][\"data\"] - \n",
    "                     features[feature_set][\"data\"][cur_instance,:]))**2\n",
    "             ).sum(axis=1))\n",
    "    # sort dataset by similarity\n",
    "    sorted_dist_idx = np.argsort(distances_l2)\n",
    "\n",
    "    print(\"This is nearest neighbor. I don't expect this to be good, because there are too many features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"Simple method to test each classifier and make sure they are working. A more robhust classification methodology would be perferable\"\"\"\n",
    "def testClassifiers():\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'KNearestNeighbor' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-49-68fbce14483d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# classifiers used for evaluation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mclassifiers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mclassifiers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"knn\"\u001b[0m\u001b[0;34m]\u001b[0m        \u001b[0;34m=\u001b[0m \u001b[0mKNearestNeighbor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_neighbors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mclassifiers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"nn\"\u001b[0m\u001b[0;34m]\u001b[0m        \u001b[0;34m=\u001b[0m \u001b[0mNeuralNetwork\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mclassifiers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"svm\"\u001b[0m\u001b[0;34m]\u001b[0m        \u001b[0;34m=\u001b[0m \u001b[0msvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSVC\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkernel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'linear'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mC\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'KNearestNeighbor' is not defined"
     ]
    }
   ],
   "source": [
    "# classifiers used for evaluation\n",
    "classifiers = {}\n",
    "classifiers[\"knn\"]        = KNearestNeighbor(n_neighbors=1)\n",
    "classifiers[\"nn\"]        = NeuralNetwork()\n",
    "classifiers[\"svm\"]        = svm.SVC(kernel='linear', C=0.3)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Data is assuming a mapping between X and y. It is not assuming the data is in feature space. This will be done later. \n",
    "\"\"\"\n",
    "class CrossValidation(object):\n",
    "    \n",
    "    def __init__(self,data):\n",
    "        self.data = data       \n",
    "        self.X = []\n",
    "        self.y = []\n",
    "        print(\"--------\", type(self.X))\n",
    "\n",
    "    \n",
    "    def KFolds(self, size):\n",
    "  \n",
    "        \n",
    "        self.size = size\n",
    "        if self.size <= 3:\n",
    "            print(\"Size must be greater than 3. You need at least 2 free folds for test and validation\")\n",
    "            return\n",
    "   \n",
    "        \n",
    "        # We have a 1 to many mapping. Need to make it a 1 to 1 mapping. \n",
    "        for label, values in self.data.items():\n",
    "            print(\"Going through \", label, \" Self x is \", type(self.X))\n",
    "            for fileid in values:\n",
    "                if(label == \"\" or fileid == \"\"):\n",
    "                    print(\"Empty Mapping Value. Please check why.\")\n",
    "                    continue\n",
    "                self.X = self.X  + [fileid] #note: Append was acting weird in this case. Not sure why. \n",
    "                self.y = self.y + [label]\n",
    "            \n",
    "        \n",
    "        # For more information on Cross Validaition using SKLearn: Please check out the documentation at http://scikit-learn.org/stable/modules/cross_validation.html\n",
    "        skf = StratifiedKFold(self.y, self.size)\n",
    "        self.skf = skf\n",
    "        # Returns the train and test indexes\n",
    "        return skf\n",
    "      \n",
    "    def Accuracy(self):\n",
    "        classifier = svm.SVC(kernel='linear', C=1)\n",
    "        scores = cross_val_score(classifier, self.X, self.y, cv=self.size)\n",
    "        print(\"Accuracy: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))\n",
    "\n",
    "    \n",
    "readSamples(base_dataset)\n",
    "cv = CrossValidation(labelmap)\n",
    "cv.KFolds(4)\n",
    "cv.Accuracy()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def testFileViewer():\n",
    "    samples, freq = readWavFileAllChannels(clean_dir_base + \"/1_clean.wav\")\n",
    "    show_stereo_waveform(samples)\n",
    "    \n",
    "#Run \n",
    "def testFouriuerViewer():\n",
    "    samples, freq = readWavFileAllChannels(clean_dir_base + \"/1_clean.wav\")\n",
    "    plotstft(samples, freq)\n",
    "\n",
    "def test():\n",
    "    testFileViewer()\n",
    "    testFouriuerViewer()\n",
    "    \n",
    "def run():\n",
    "    test()\n",
    "    print(\"Running\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'clean_dir_base' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-157c9bda2cd6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-16-b6d069ba69ec>\u001b[0m in \u001b[0;36mrun\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Running\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-16-b6d069ba69ec>\u001b[0m in \u001b[0;36mtest\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0mtestFileViewer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0mtestFouriuerViewer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-16-b6d069ba69ec>\u001b[0m in \u001b[0;36mtestFileViewer\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtestFileViewer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0msamples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfreq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreadWavFileAllChannels\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclean_dir_base\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"/1_clean.wav\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mshow_stereo_waveform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m#Run\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'clean_dir_base' is not defined"
     ]
    }
   ],
   "source": [
    "run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
